% This contents of this file will be inserted into the _Solutions version of the
% output tex document.  Here's an example:

% If assignment with subquestion (1.a) requires a written response, you will
% find the following flag within this document: <SCPD_SUBMISSION_TAG>_1a
% In this example, you would insert the LaTeX for your solution to (1.a) between
% the <SCPD_SUBMISSION_TAG>_1a flags.  If you also constrain your answer between the
% START_CODE_HERE and END_CODE_HERE flags, your LaTeX will be styled as a
% solution within the final document.

% Please do not use the '<SCPD_SUBMISSION_TAG>' character anywhere within your code.  As expected,
% that will confuse the regular expressions we use to identify your solution.

\def\assignmentnum{2 }
% \def\assignmentname{Sentiment Analysis}
\def\assignmenttitle{XCS234 Assignment \assignmentnum}
\input{macros}
\begin{document}
\pagestyle{myheadings} \markboth{}{\assignmenttitle}

% <SCPD_SUBMISSION_TAG>_entire_submission

This handout includes space for every question that requires a written response.
Please feel free to use it to handwrite your solutions (legibly, please).  If
you choose to typeset your solutions, the $\mid$ README.md $\mid$ for this assignment includes
instructions to regenerate this handout with your typeset \LaTeX{} solutions.
\ruleskip

\LARGE
1.b
\normalsize

% <SCPD_SUBMISSION_TAG>_1b
\begin{answer}
  % ### START CODE HERE ###
  Based on Jensen's inequality, we have:
  $$
  \mathbb{E}[f(X)] \ge f(\mathbb{E}[X])
  $$
  for random variable $X$ and a convex function $f$.

  The maximum function is a convex function and we consider Q-values for all actions in a given state as a random variable. Applying Jensen's inequality, we get:
  $$
  \mathbb{E}[\max_{a \in \mathcal{A}} Q(s,a)] \ge \max_{a \in \mathcal{A}} (\mathbb{E}[Q(s,a)])
  $$

  We are given that $\mathbb{E}[Q(s,a)] = Q^*(s,a)$, so
  $$
  \mathbb{E}[\max_{a \in \mathcal{A}}Q(s,a)] \ge \max_{a \in \mathcal{A}}Q^{*}(s,a)
  $$
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1b
\clearpage


\LARGE
2.a
\normalsize

% <SCPD_SUBMISSION_TAG>_2a
\begin{answer}
  % ### START CODE HERE ###
  The partial derivative of a dot product is simply the other vector in the dot product (i.e. $\partial (u \cdot v) / \partial u = v)$), so
  $$
  \nabla_{\theta}Q_{\theta}(s,a) = \frac{\partial Q_{\theta}(s,a)}{\partial \theta_{s,a}} = \frac{\partial \theta_{s,a} \cdot \delta(s,a)}{\partial \theta_{s,a}} = \delta(s,a)
  $$

  Substitute this and into the linear update equation:
  $$
  \theta \leftarrow \theta + \alpha(r + \gamma \max_{a'}Q_{\theta}(s',a') - Q_{\theta}(s,a))\nabla_{\theta}Q_{\theta}(s,a)
  $$

  We get:
  $$
  \theta \leftarrow \theta + \alpha(r + \gamma \max_{a'}Q_{\theta}(s',a') - Q_{\theta}(s,a))\delta(s,a)
  $$

  Shows that only the single $\theta$ component correspond to $(s,a)$ will be updated and thus identical to:
  $$
  \theta \cdot \delta(s,a) \leftarrow \theta \cdot \delta(s,a) + \alpha(r + \gamma \max_{a'}Q_{\theta}(s',a') - Q_{\theta}(s,a))\delta(s,a)
  $$

  And therefore
  $$
  Q_{\theta}(s,a) \leftarrow Q_{\theta}(s,a) + \alpha(r + \gamma \max_{a'}Q_{\theta}(s',a') - Q_{\theta}(s,a))
  $$
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2a
\clearpage


\LARGE
3.c
\normalsize

% <SCPD_SUBMISSION_TAG>_3c
\begin{answer}
  % ### START CODE HERE ###
  The linear model is obviously learn quicker and give you more stable results while DQN can
  sometimes fail to reach the optimal policy with the epoch we choose. DQN might need even
  more training steps to reach the optimal policy.

  In conclusion, for the simple test environment, the linear model is the better choice due
  to its efficiency and appropriate complexity. The DQN is unnecessarily powerful for this task.
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_3c
\clearpage

\LARGE
4.a
\normalsize

% <SCPD_SUBMISSION_TAG>_4a
\begin{answer}
  % ### START CODE HERE ###
  \begin{figure}[H]
  \centering
    \includegraphics[width=.5\linewidth]{images/q4_linear.png}
  \end{figure}
  The performance of the linear model on the Pong-v5 environment is extremely poor and shows
  no evidence of learning.
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_4a
\clearpage

\LARGE
4.c
\normalsize

% <SCPD_SUBMISSION_TAG>_4c
\begin{answer}
  % ### START CODE HERE ###
  The DQN is obviously improving its performance over time while the linear model doesn't seem
  to improve.

  The DQN has a deep CNN layer that are specifically designed for image data. It gives the DQN
  the understanding of the game state directly from pixels, which is necessary for high-level
  strategic play.

  On the other hand, the linear model treat each pixels as an independent feature and attempts
  to find a simple linear relationship between pixel values and strategy, which is insufficient.
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_4c
\clearpage

\LARGE
4.d
\normalsize

% <SCPD_SUBMISSION_TAG>_4d
\begin{answer}
  % ### START CODE HERE ###
  No, the performance of DQN is improving in the long run but it fluctuate a lot locally.
  It could be the $\epsilon$-greddy exploration where it randomly choose the actions sometimes
  that causes the fluctuation. Also, the deep neural network is a highly non-linear function
  approximation. A small parameter change can lead to significant change in the policy.
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_4d
\clearpage

% <SCPD_SUBMISSION_TAG>_entire_submission

\end{document}
